# E2E-MERA-SC
This is the open source code for paper: Open source code for paper: End-to-End Emotion Recognition Method Based on Multimodal Information from Voice, Text, and Images
## Paper Abstract 

> The burgeoning prevalence of smart cities and smart homes heralds an intensifying interaction between humans and technology. Against this backdrop, crafting a more intuitive and natural interactive experience has become paramount. In sectors such as smart healthcare and intelligent transportation, accurately capturing and comprehending human emotions is crucial. Yet, most of today’s emotion recognition techniques lean heavily on singular modalities, undoubtedly constraining their depth and breadth in capturing emotions, especially in intricate settings. Multimodal emotion recognition, amalgamating in-depth analyses of images, voice, and text, is emerging as a frontrunner. Given these considerations, to further the widespread application of multimodal emotion recognition in smart city domains, including smart healthcare, product recommendations, and smart homes, we have devised an innovative end-to-end multimodal emotion recognition framework. This encompasses three pivotal dimensions: voice, text, and image. To streamline the manual text input phase within the multimodal system, we have integrated automatic speech recognition technology, translating voice into real-time text labels. Additionally, for voice, image, and text, we have engineered an efficient end-to-end feature extraction approach. To delve deeper into the synergistic value across modalities, we’ve also incorporated a self-supervised multi-task learning strategy. Ensuring its efficacy in real-world scenarios, we furnish a comprehensive deployment and implementation guide. Rigorous experimental validation underscores the exemplary performance of this framework, highlighting its vast application potential.

If you are interested in our work, please contact zhuxianxun@shu.edu.cn  

## Preparation
### Datasets
As mentioned in our paper, in order to train our model, you need to download the CH-SIMS model here: [CH-SIMS](https://drive.google.com/drive/folders/1A2S4pqCHryGmiqnNSPLv7rEg63WvjCSk).
