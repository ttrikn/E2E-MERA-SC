# E2E-MERA-SC
Codes for End-to-End Emotion Recognition Method Based on Multimodal Information from Voice, Text, and Images
## Paper Abstract 

> The rapid rise of smart cities and smart homes signifies an ever-increasing inter- action between humans and technology. In such environments, providing a more natural and emotional interaction experience is becoming crucial. In fields such as smart healthcare and smart transportation, accurately identifying and under- standing human emotions is key to delivering high-quality services. Currently, the majority of emotion recognition is still focused on a single modality, which undoubtedly limits its capability for a deep and comprehensive understanding of emotions, especially in identifying individual emotions in complex scenar- ios. Multi-modal emotion recognition, integrating in-depth analysis of images, voice, and text data, is gradually becoming the central strategy for identifying human emotions. In light of this, we introduce an innovative multi-modal emo- tion recognition framework, encompassing the three major dimensions of voice, text, and image. Firstly, we employ state-of-the-art automatic speech recogni- tion technology and speech-to-text schemes to automatically convert speech in videos into text labels, thus eliminating the tedious manual text input steps. Sec- ondly, for voice, image, and text, weâ€™ve constructed an efficient end-to-end feature extraction approach. To deeply tap into the synergistic effects between different modalities, we introduce a self-supervised multi-task learning strategy for data fusion. Experimental validation showed that this approach performs outstand- ingly, with a broad range of future applications.

If you are interested in our work, please contact zhuxianxun@shu.edu.cn  
